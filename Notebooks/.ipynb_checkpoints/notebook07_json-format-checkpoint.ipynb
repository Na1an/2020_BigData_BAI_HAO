{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `JSON` data with `Python`\n",
    "\n",
    "First, let's use `Python` to serialize and deserialize data using `JSON` for data containing only built-in types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization and deserialization of built-in types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T10:07:56.680689Z",
     "start_time": "2020-03-24T10:07:56.667534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"active\": true,\n",
      "    \"age\": 78,\n",
      "    \"balance\": 345.8,\n",
      "    \"friends\": [\n",
      "        \"Jane\",\n",
      "        \"John\"\n",
      "    ],\n",
      "    \"name\": \"Foo Bar\",\n",
      "    \"other_names\": [\n",
      "        \"Doe\",\n",
      "        \"Joe\"\n",
      "    ],\n",
      "    \"spouse\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "obj = {\n",
    "    \"name\": \"Foo Bar\",\n",
    "    \"age\": 78,\n",
    "    \"friends\": [\"Jane\",\"John\"],\n",
    "    \"balance\": 345.80,\n",
    "    \"other_names\":(\"Doe\",\"Joe\"),\n",
    "    \"active\": True,\n",
    "    \"spouse\": None\n",
    "}\n",
    "#indent 缩进\n",
    "print(json.dumps(obj, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T10:08:07.076896Z",
     "start_time": "2020-03-24T10:08:07.072075Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('user.json','w') as file:\n",
    "    json.dump(obj, file, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T10:08:10.048257Z",
     "start_time": "2020-03-24T10:08:09.586178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"active\": true,\r\n",
      "    \"age\": 78,\r\n",
      "    \"balance\": 345.8,\r\n",
      "    \"friends\": [\r\n",
      "        \"Jane\",\r\n",
      "        \"John\"\r\n",
      "    ],\r\n",
      "    \"name\": \"Foo Bar\",\r\n",
      "    \"other_names\": [\r\n",
      "        \"Doe\",\r\n",
      "        \"Joe\"\r\n",
      "    ],\r\n",
      "    \"spouse\": null\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "#有很多行\n",
    "!cat user.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:27.137307Z",
     "start_time": "2020-03-17T14:29:27.114179Z"
    }
   },
   "outputs": [],
   "source": [
    "json.loads('{\"active\": true, \"age\": 78, \"balance\": 345.8, \"friends\": [\"Jane\",\"John\"], \"name\": \"Foo Bar\", \"other_names\": [\"Doe\",\"Joe\"],\"spouse\":null}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:27.149816Z",
     "start_time": "2020-03-17T14:29:27.140548Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('user.json', 'r') as file:\n",
    "    user_data = json.load(file)\n",
    "\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization and deserialization of custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:27.733203Z",
     "start_time": "2020-03-17T14:29:27.718399Z"
    }
   },
   "outputs": [],
   "source": [
    "class User(object):\n",
    "    \"\"\"Custom User Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name, age, active, balance, \n",
    "                 other_names, friends, spouse):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.active = active\n",
    "        self.balance = balance\n",
    "        self.other_names = other_names\n",
    "        self.friends = friends\n",
    "        self.spouse = spouse\n",
    "            \n",
    "    def __repr__(self):\n",
    "        s = \"User(\"\n",
    "        s += \"name=\" + repr(self.name)\n",
    "        s += \", age=\" + repr(self.age)\n",
    "        s += \", active=\" + repr(self.active)\n",
    "        s += \", other_names=\" + repr(self.other_names)\n",
    "        s += \", friends=\" + repr(self.friends)\n",
    "        s += \", spouse=\" + repr(self.spouse) + \")\"\n",
    "        return s\n",
    "\n",
    "new_user = User(\n",
    "    name = \"Foo Bar\",\n",
    "    age = 78,\n",
    "    friends = [\"Jane\", \"John\"],\n",
    "    balance = 345.80,\n",
    "    other_names = (\"Doe\", \"Joe\"),\n",
    "    active = True,\n",
    "    spouse = None\n",
    ")\n",
    "\n",
    "new_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:49.613442Z",
     "start_time": "2020-03-17T14:29:49.601102Z"
    }
   },
   "outputs": [],
   "source": [
    "# This will raise a TypeError\n",
    "json.dumps(new_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the custom object `new_user` is not JSON serializable. So let's build a method that does that for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I bet this comes as no surprise to us, since earlier on we established that\n",
    "the json module only understands the built-in types, and User is not one of those.\n",
    "\n",
    "- We need to send our user data to a client over anetwork, so how do we get \n",
    "ourselves out of this error state?\n",
    "\n",
    "- A simple solution would be to convert our custom type in to a serializable\n",
    "type — i.e a built-in type. We can conveniently define a method convert_to_dict() \n",
    "that returns a dictionary representation of our object. json.dumps() \n",
    "takes in a optional argument, default , which specifies a function to be called if the object is not serializable. This function returns a JSON encodable version of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:52.854067Z",
     "start_time": "2020-03-17T14:29:52.847015Z"
    }
   },
   "outputs": [],
   "source": [
    "def obj_to_dict(obj):\n",
    "    \"\"\"Converts an object to a dictionary representation of the object including \n",
    "    meta-data information about the object's module and class name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : `object`\n",
    "        A python object to be converted into a dictionary representation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : `dict`\n",
    "        A dictionary representation of the object\n",
    "    \"\"\"\n",
    "    # Add object meta data \n",
    "    obj_dict = {\n",
    "        \"__class__\": obj.__class__.__name__,\n",
    "        \"__module__\": obj.__module__\n",
    "    }\n",
    "    # Add the object properties\n",
    "    obj_dict.update(obj.__dict__)\n",
    "    return obj_dict\n",
    "\n",
    "obj_to_dict(new_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `convert_to_dict` does the following:\n",
    "\n",
    "- We then create a dictionary named obj_dict to act as the dict representation of our object.\n",
    "\n",
    "- The magic methods `__class__.__name__` and `__module__` provide crucial metadata on the object: the class name and the module name\n",
    "\n",
    "- We add the instance attributes of the object using `obj.__dict__` (`Python` stores instance attributes in a dictionary under the hood)\n",
    "\n",
    "- The resulting `dict` is now serializable\n",
    "\n",
    "Now we can comfortably call `json.dumps()` on the object and pass `default=convert_to_dict`\n",
    "\n",
    "**Remark.** Obviously this fails if one of the attributes is not `JSON` serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:53.566068Z",
     "start_time": "2020-03-17T14:29:53.560201Z"
    }
   },
   "outputs": [],
   "source": [
    "print(json.dumps(new_user, default=obj_to_dict, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to decode (deserialiaze) a custom object, and create the correct object type, we need a function that does the opposite of convert_to_dict, since `json.loads` simply returns a `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:54.413286Z",
     "start_time": "2020-03-17T14:29:54.408660Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_data = json.loads(json.dumps(new_user, default=obj_to_dict))\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `json.loads()` to reconstruct a `User` object from this dictionary: `json.loads()`\n",
    "takes an optional argument `object_hook` which specifies a function that returns the desired custom object, given the decoded output (which in this case is a `dict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:55.257839Z",
     "start_time": "2020-03-17T14:29:55.253496Z"
    }
   },
   "outputs": [],
   "source": [
    "def dict_to_obj(input_dict):\n",
    "    \"\"\"Converts a dictionary representation of an object to an instance of the object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dict : `dict`\n",
    "        A dictionary representation of the object, containing \"__module__\" \n",
    "        and \"__class__\" metadata\n",
    "\n",
    "    Returns\n",
    "    -------    \n",
    "    obj : `object`\n",
    "        A python object constructed from the dictionary representation    \n",
    "    \"\"\"\n",
    "    assert \"__class__\" in input_dict and \"__module__\" in input_dict\n",
    "    class_name = input_dict.pop(\"__class__\")\n",
    "    module_name = input_dict.pop(\"__module__\")\n",
    "    module = __import__(module_name)\n",
    "    class_ = getattr(module, class_name)\n",
    "    obj = class_(**input_dict)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the following: \n",
    "\n",
    "- Extract the class name from the dictionary under the key `__class__`\n",
    "\n",
    "- Extract the module name from the dictionary under the key `__module__`\n",
    "\n",
    "- Imports the module and get the class\n",
    "\n",
    "- Instantiate the class by giving to the class constructor all the instance arguments through dictionary unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:57.102787Z",
     "start_time": "2020-03-17T14:29:57.097553Z"
    }
   },
   "outputs": [],
   "source": [
    "obj_data = json.dumps(new_user, default=obj_to_dict)\n",
    "new_object = json.loads(obj_data, object_hook=dict_to_obj)\n",
    "new_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:57.944835Z",
     "start_time": "2020-03-17T14:29:57.939581Z"
    }
   },
   "outputs": [],
   "source": [
    "type(new_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T14:29:58.679143Z",
     "start_time": "2020-03-17T14:29:58.673617Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_object.age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `JSON` with Spark\n",
    "\n",
    "First, we download the data if it's not there yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T16:19:34.238130Z",
     "start_time": "2020-03-17T16:19:34.163729Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path('drug-enforcement.json')\n",
    "if not path.exists():\n",
    "    url = \"https://stephanegaiffas.github.io/big_data_course/data/drug-enforcement.json.zip\"\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a `JSON` dataset with `Spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/interactively-analyse-100gb-of-json-data-with-spark-e018f9436e76\n",
    "\n",
    "- https://medium.com/@clementselvaraj/https-medium-com-querying-json-in-python-using-spark-sql-a08761946dd2\n",
    "\n",
    "- https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html\n",
    "\n",
    "- https://towardsdatascience.com/interactively-analyse-100gb-of-json-data-with-spark-e018f9436e76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:13:47.583384Z",
     "start_time": "2020-03-17T17:13:43.123090Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "conf = SparkConf().setAppName(\"Spark JSON\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark JSON\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:13:47.590453Z",
     "start_time": "2020-03-17T17:13:47.585590Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"drug-enforcement.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets look at the data. It's a large set of JSON records about drugs enforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:13:48.148481Z",
     "start_time": "2020-03-17T17:13:47.594201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 1000 drug-enforcement.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell spark that rows span on several lines with the `multLine` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:13:52.259661Z",
     "start_time": "2020-03-17T17:13:48.152192Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(filename, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:13:52.294898Z",
     "start_time": "2020-03-17T17:13:52.261550Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:13:52.355556Z",
     "start_time": "2020-03-17T17:13:52.297210Z"
    }
   },
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a little bit of a mess ! \n",
    "\n",
    "- First, there is a nested `opendfa` dictionary. Each element of the dictionary is an array\n",
    "- A first good idea is to **\"flatten\" the schema of the DataFrame**, so that there is no nested types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening of the schema\n",
    "\n",
    "All the columns in the nested structure `openfda` are put up in the schema.\n",
    "These columns nested in the openfda are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:14:00.472548Z",
     "start_time": "2020-03-17T17:14:00.352058Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select('openfda.*').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:14:01.904242Z",
     "start_time": "2020-03-17T17:14:00.646360Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"openfda.*\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:14:02.230811Z",
     "start_time": "2020-03-17T17:14:01.906641Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in df.select(\"openfda.*\").columns:\n",
    "    df = df.withColumn(\"openfda_\" + c, col(\"openfda.\" + c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:14:07.644985Z",
     "start_time": "2020-03-17T17:14:07.501380Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.select([c for c in df.columns if c != \"openfda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:14:09.871658Z",
     "start_time": "2020-03-17T17:14:09.865537Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:14:19.204515Z",
     "start_time": "2020-03-17T17:14:18.651165Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the display of the `DataFrame` is not as usual... it displays the dataframe like a list of `Row`, since the columns \"openfda*\" contain arrays of varying length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "A strategy can be to remove rows with missing data. \n",
    "`dropna` has several options, explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:17.413952Z",
     "start_time": "2020-03-17T17:41:16.898211Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove all lines with at least one missing value, we end up with an empty dataframe !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:17.895108Z",
     "start_time": "2020-03-17T17:41:17.416108Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(how='all').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dropna` accepts the following arguments\n",
    "\n",
    "- `how`: can be `'any'` or `'all'`. If `'any'`, rows containing any null values will be dropped entirely (this is the default). If `'all'`, only rows which are entirely empty will be dropped.\n",
    "\n",
    "- `thresh`: accepts an integer representing the \"threshold\" for how many empty cells a row must have before being dropped. `tresh` is a middle ground between `how='any'` and `how='all'`. As a result, the presence of `thresh` will override `how`\n",
    "\n",
    "- `subset`: accepts a list of column names. When a subset is present, N/A values will only be checked against the columns whose names are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:17.901172Z",
     "start_time": "2020-03-17T17:41:17.897860Z"
    }
   },
   "outputs": [],
   "source": [
    "n_columns = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:18.415066Z",
     "start_time": "2020-03-17T17:41:17.904093Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh=n_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:18.931320Z",
     "start_time": "2020-03-17T17:41:18.417317Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh=n_columns-1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:19.439726Z",
     "start_time": "2020-03-17T17:41:18.934217Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh=n_columns-10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:19.752318Z",
     "start_time": "2020-03-17T17:41:19.441809Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['postal_code', 'city', 'country', 'address_1'])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before this, let's count the number of missing value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:20.424178Z",
     "start_time": "2020-03-17T17:41:19.754255Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each column we create a new column containing 1 if the value is null and 0 otherwise.\n",
    "# We need to bast Boolean to Int so that we can use fn.sum after\n",
    "for c in df.columns:\n",
    "    # Do not do this for _isnull columns (ince case you run this cell twice...)\n",
    "    if not c.endswith(\"_isnull\"):\n",
    "        df = df.withColumn(c + \"_isnull\", fn.isnull(col(c)).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:20.925299Z",
     "start_time": "2020-03-17T17:41:20.426204Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:22.053794Z",
     "start_time": "2020-03-17T17:41:20.927796Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the list of _isnull columns\n",
    "isnull_columns = [c for c in df.columns if c.endswith(\"_isnull\")]\n",
    "\n",
    "# On the _isnull columns :\n",
    "#  - we compute the sum to have the number of null values and rename the column\n",
    "#  - convert to pandas for better readability\n",
    "#  - transpose the pandas dataframe for better readability\n",
    "missing_values = df.select(isnull_columns)\\\n",
    "    .agg(*[fn.sum(c).alias(c.replace(\"_isnull\", \"\")) for c in isnull_columns])\\\n",
    "    .toPandas()\n",
    "\n",
    "missing_values.T\\\n",
    "    .rename({0: \"missing values\"}, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `more_code_info` is always null and that `termination_date` if often null. \n",
    "Most of the `openfda*` columns are also almost always empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep only the columns with no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:24.802336Z",
     "start_time": "2020-03-17T17:41:24.797411Z"
    }
   },
   "outputs": [],
   "source": [
    "# This line can seem complicated, run pieces of each to understand\n",
    "kept_columns = list(\n",
    "    missing_values.columns[(missing_values.iloc[0] == 0).values]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:27.488388Z",
     "start_time": "2020-03-17T17:41:27.398503Z"
    }
   },
   "outputs": [],
   "source": [
    "df_kept = df.select(kept_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:27.948018Z",
     "start_time": "2020-03-17T17:41:27.539797Z"
    }
   },
   "outputs": [],
   "source": [
    "df_kept.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:41:41.419929Z",
     "start_time": "2020-03-17T17:41:41.413784Z"
    }
   },
   "outputs": [],
   "source": [
    "df_kept.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:43:23.207185Z",
     "start_time": "2020-03-17T17:43:22.829140Z"
    }
   },
   "outputs": [],
   "source": [
    "df_kept.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T18:01:14.687898Z",
     "start_time": "2020-03-14T18:01:14.682667Z"
    }
   },
   "source": [
    "## Filtering by string values \n",
    "\n",
    "Cases from South San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:43:54.396689Z",
     "start_time": "2020-03-17T17:43:53.465012Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df.city == \"South San Francisco\")\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.** Once again, we use `.toPandas()` to pretty format the results in the notebook. \n",
    "But it's a BAD idea to do this if the spark DataFrame is large, since it requires a `collect()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from filtering strings by a perfect match, there are plenty of other powerful ways to filter by strings in `pyspark` :\n",
    "\n",
    "- `df.filter(df.city.contains('San Francisco'))`: returns rows where strings of a column contain a provided substring. In our example, filtering by rows which contain the substring \"San Francisco\" would be a good way to get all rows in San Francisco, instead of just \"South San Francisco\".\n",
    "\n",
    "- `df.filter(df.city.startswith('San'))`: Returns rows where a string starts with a provided substring.\n",
    "\n",
    "- `df.filter(df.city.endswith('ice'))`: Returns rows where a string starts with a provided substring.\n",
    "\n",
    "- `df.filter(df.city.isNull())`: Returns rows where values in a provided column are null.\n",
    "\n",
    "- `df.filter(df.city.isNotNull())`: Opposite of the above.\n",
    "\n",
    "- `df.filter(df.city.like('San%'))`: Performs a SQL-like query containing the LIKE clause.\n",
    "\n",
    "- `df.filter(df.city.rlike('[A-Z]*ice$'))`: Performs a regexp filter.\n",
    "\n",
    "- `df.filter(df.city.isin('San Francisco', 'Los Angeles'))`: Looks for rows where the string value of a column matches any of the provided strings exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try some of these to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:48:19.328007Z",
     "start_time": "2020-03-17T17:48:18.572570Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df.city.contains('San Francisco'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:48:10.490019Z",
     "start_time": "2020-03-17T17:48:09.784496Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df.city.isin('San Francisco', 'Los Angeles')).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by Date Values\n",
    "\n",
    "In addition to filtering by strings, we can also filter by columns where the values are stored as dates or datetimes (or strings that can be inferred as dates). Perhaps the most useful way to filter dates is by using the `between()` method, which allows us to find results within a certain date range. Here we find all the results which were reported in the years 2013 and 2014:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:50:34.344304Z",
     "start_time": "2020-03-17T17:50:33.638867Z"
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df.city == \"South San Francisco\")\\\n",
    "    .filter(df.report_date.between('2013-01-01 00:00:00','2015-01-11 00:00:00'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is smart and understand that the string contains a date actually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.4375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
